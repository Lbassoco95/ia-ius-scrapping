#!/usr/bin/env python3
"""
Script para analizar la estructura de la p√°gina de la SCJN
y entender por qu√© no se encuentran resultados
"""

import logging
import sys
import os
import time
from datetime import datetime

# Configurar logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

# Agregar el directorio ra√≠z al path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from src.scraper.selenium_scraper import SeleniumSCJNScraper

def analyze_page_structure():
    """Analizar la estructura de la p√°gina de la SCJN"""
    
    print("üîç === AN√ÅLISIS DE LA P√ÅGINA DE LA SCJN ===")
    
    scraper = SeleniumSCJNScraper()
    
    try:
        # Configurar driver
        if not scraper.setup_driver():
            print("‚ùå No se pudo configurar el driver")
            return False
        
        # Navegar a la p√°gina
        if not scraper.navigate_to_search_page():
            print("‚ùå No se pudo navegar a la p√°gina")
            scraper.close_driver()
            return False
        
        print("‚úÖ P√°gina cargada correctamente")
        
        # Analizar estructura de la p√°gina
        print("\nüìã Analizando estructura de la p√°gina...")
        
        # Obtener t√≠tulo de la p√°gina
        page_title = scraper.driver.title
        print(f"üìÑ T√≠tulo de la p√°gina: {page_title}")
        
        # Obtener URL actual
        current_url = scraper.driver.current_url
        print(f"üåê URL actual: {current_url}")
        
        # Buscar elementos de b√∫squeda
        print("\nüîç Analizando elementos de b√∫squeda...")
        
        # Buscar campos de entrada
        input_elements = scraper.driver.find_elements("tag name", "input")
        print(f"üìù Campos de entrada encontrados: {len(input_elements)}")
        
        for i, elem in enumerate(input_elements[:5]):  # Mostrar solo los primeros 5
            try:
                input_type = elem.get_attribute("type")
                input_placeholder = elem.get_attribute("placeholder")
                input_id = elem.get_attribute("id")
                input_class = elem.get_attribute("class")
                print(f"  Input {i+1}: type={input_type}, placeholder='{input_placeholder}', id='{input_id}', class='{input_class}'")
            except:
                continue
        
        # Buscar botones
        button_elements = scraper.driver.find_elements("tag name", "button")
        print(f"\nüîò Botones encontrados: {len(button_elements)}")
        
        for i, elem in enumerate(button_elements[:5]):  # Mostrar solo los primeros 5
            try:
                button_text = elem.text
                button_id = elem.get_attribute("id")
                button_class = elem.get_attribute("class")
                print(f"  Bot√≥n {i+1}: text='{button_text}', id='{button_id}', class='{button_class}'")
            except:
                continue
        
        # Buscar enlaces
        link_elements = scraper.driver.find_elements("tag name", "a")
        print(f"\nüîó Enlaces encontrados: {len(link_elements)}")
        
        # Filtrar enlaces que contengan "tesis"
        tesis_links = []
        for elem in link_elements:
            try:
                href = elem.get_attribute("href")
                text = elem.text
                if href and ("tesis" in href.lower() or "tesis" in text.lower()):
                    tesis_links.append((text, href))
            except:
                continue
        
        print(f"üìö Enlaces relacionados con tesis: {len(tesis_links)}")
        for text, href in tesis_links[:10]:  # Mostrar solo los primeros 10
            print(f"  üìÑ '{text}' -> {href}")
        
        # Buscar elementos con clases espec√≠ficas
        print("\nüé® Analizando clases CSS...")
        
        # Buscar elementos con clase "list-group-item"
        list_items = scraper.driver.find_elements("css selector", ".list-group-item")
        print(f"üìã Elementos .list-group-item: {len(list_items)}")
        
        # Buscar elementos con clase "item"
        items = scraper.driver.find_elements("css selector", ".item")
        print(f"üìã Elementos .item: {len(items)}")
        
        # Buscar elementos con clase "result"
        results = scraper.driver.find_elements("css selector", ".result")
        print(f"üìã Elementos .result: {len(results)}")
        
        # Buscar tablas
        tables = scraper.driver.find_elements("tag name", "table")
        print(f"üìä Tablas encontradas: {len(tables)}")
        
        # Buscar filas de tabla
        rows = scraper.driver.find_elements("tag name", "tr")
        print(f"üìä Filas de tabla: {len(rows)}")
        
        # Intentar hacer una b√∫squeda manual
        print("\nüîç Intentando b√∫squeda manual...")
        
        # Buscar campo de b√∫squeda
        search_input = None
        search_selectors = [
            "input[type='text']",
            "input[type='search']",
            "input[placeholder*='buscar']",
            "input[placeholder*='Buscar']",
            "input[placeholder*='tesis']",
            "input[placeholder*='Tesis']",
            "#search",
            ".search-input",
            "input.form-control"
        ]
        
        for selector in search_selectors:
            try:
                elements = scraper.driver.find_elements("css selector", selector)
                for elem in elements:
                    if elem.is_displayed() and elem.is_enabled():
                        search_input = elem
                        print(f"‚úÖ Campo de b√∫squeda encontrado con selector: {selector}")
                        break
                if search_input:
                    break
            except:
                continue
        
        if search_input:
            # Intentar escribir en el campo
            try:
                search_input.clear()
                search_input.send_keys("amparo")
                print("‚úÖ Texto escrito en campo de b√∫squeda")
                
                # Buscar bot√≥n de b√∫squeda
                search_button = None
                button_selectors = [
                    "button[type='submit']",
                    "input[type='submit']",
                    "button:contains('Buscar')",
                    "button:contains('buscar')",
                    ".search-button",
                    "#search-button",
                    "button.btn"
                ]
                
                for selector in button_selectors:
                    try:
                        elements = scraper.driver.find_elements("css selector", selector)
                        for elem in elements:
                            if elem.is_displayed() and elem.is_enabled():
                                search_button = elem
                                print(f"‚úÖ Bot√≥n de b√∫squeda encontrado con selector: {selector}")
                                break
                        if search_button:
                            break
                    except:
                        continue
                
                if search_button:
                    # Hacer click en el bot√≥n
                    search_button.click()
                    print("‚úÖ Bot√≥n de b√∫squeda clickeado")
                    
                    # Esperar resultados
                    time.sleep(5)
                    
                    # Analizar p√°gina despu√©s de la b√∫squeda
                    print("\nüìä Analizando resultados de b√∫squeda...")
                    
                    # Buscar elementos de resultado
                    result_elements = scraper.driver.find_elements("css selector", ".list-group-item")
                    print(f"üìã Elementos .list-group-item despu√©s de b√∫squeda: {len(result_elements)}")
                    
                    # Mostrar contenido de los primeros elementos
                    for i, elem in enumerate(result_elements[:3]):
                        try:
                            text = elem.text
                            print(f"  Elemento {i+1}: {text[:100]}...")
                        except:
                            continue
                    
                else:
                    print("‚ö†Ô∏è No se encontr√≥ bot√≥n de b√∫squeda")
                    
            except Exception as e:
                print(f"‚ùå Error en b√∫squeda manual: {e}")
        else:
            print("‚ö†Ô∏è No se encontr√≥ campo de b√∫squeda")
        
        # Obtener HTML de la p√°gina para an√°lisis
        print("\nüìÑ Obteniendo HTML de la p√°gina...")
        page_source = scraper.driver.page_source
        
        # Guardar HTML para an√°lisis
        with open("logs/page_analysis.html", "w", encoding="utf-8") as f:
            f.write(page_source)
        print("‚úÖ HTML guardado en logs/page_analysis.html")
        
        # Buscar patrones espec√≠ficos en el HTML
        print("\nüîç Analizando patrones en HTML...")
        
        # Buscar patrones de tesis
        tesis_patterns = [
            "detalle/tesis",
            "tesis/",
            "tesis",
            "jurisprudencia",
            "amparo",
            "constitucional"
        ]
        
        for pattern in tesis_patterns:
            count = page_source.lower().count(pattern.lower())
            print(f"  Patr√≥n '{pattern}': {count} ocurrencias")
        
        print("\n‚úÖ An√°lisis completado")
        
    except Exception as e:
        print(f"‚ùå Error en an√°lisis: {e}")
    finally:
        scraper.close_driver()
    
    return True

def main():
    """Funci√≥n principal"""
    success = analyze_page_structure()
    
    if success:
        print("\nüìã Resumen del an√°lisis:")
        print("‚úÖ Se analiz√≥ la estructura de la p√°gina")
        print("‚úÖ Se identificaron elementos de b√∫squeda")
        print("‚úÖ Se guard√≥ el HTML para an√°lisis detallado")
        print("üìÅ Revisa logs/page_analysis.html para m√°s detalles")
    else:
        print("\n‚ùå Error en el an√°lisis")

if __name__ == "__main__":
    main() 