#!/usr/bin/env python3
"""
Script de debug para capturar la pÃ¡gina despuÃ©s de hacer click en "Ver todo"
"""

import logging
import sys
import os
import time
from datetime import datetime

# Configurar logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

# Agregar el directorio raÃ­z al path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from src.scraper.selenium_scraper import SeleniumSCJNScraper

def debug_search_flow():
    """Debug del flujo de bÃºsqueda completo"""
    
    print("ğŸ” === DEBUG DEL FLUJO DE BÃšSQUEDA ===")
    
    scraper = SeleniumSCJNScraper()
    
    try:
        # Configurar driver
        if not scraper.setup_driver():
            print("âŒ No se pudo configurar el driver")
            return False
        
        # Navegar a la pÃ¡gina
        if not scraper.navigate_to_search_page():
            print("âŒ No se pudo navegar a la pÃ¡gina")
            scraper.close_driver()
            return False
        
        print("âœ… PÃ¡gina cargada correctamente")
        
        # 1. Buscar campo de bÃºsqueda
        print("\n1ï¸âƒ£ Buscando campo de bÃºsqueda...")
        search_input = None
        search_selectors = [
            "input[type='text']",
            "input[placeholder*='Escriba el tema']",
            "input.form-control.sjf-input-search"
        ]
        
        for selector in search_selectors:
            try:
                elements = scraper.driver.find_elements("css selector", selector)
                for elem in elements:
                    if elem.is_displayed() and elem.is_enabled():
                        search_input = elem
                        print(f"âœ… Campo de bÃºsqueda encontrado con selector: {selector}")
                        break
                if search_input:
                    break
            except:
                continue
        
        if not search_input:
            print("âŒ No se encontrÃ³ campo de bÃºsqueda")
            return False
        
        # 2. Escribir tÃ©rmino de bÃºsqueda
        print("\n2ï¸âƒ£ Escribiendo tÃ©rmino de bÃºsqueda...")
        search_input.clear()
        search_input.send_keys("amparo")
        print("âœ… Texto escrito: 'amparo'")
        
        # 3. Buscar botÃ³n de bÃºsqueda
        print("\n3ï¸âƒ£ Buscando botÃ³n de bÃºsqueda...")
        search_button = None
        button_selectors = [
            "button.btn.sjf-button-search",
            "button[class*='btn']",
            "button:contains('Ver todo')",
            "#button-addon1_add"
        ]
        
        for selector in button_selectors:
            try:
                elements = scraper.driver.find_elements("css selector", selector)
                for elem in elements:
                    if elem.is_displayed() and elem.is_enabled():
                        button_text = elem.text.strip()
                        if "Ver todo" in button_text or "Buscar" in button_text:
                            search_button = elem
                            print(f"âœ… BotÃ³n encontrado: '{button_text}' con selector: {selector}")
                            break
                if search_button:
                    break
            except:
                continue
        
        if not search_button:
            print("âŒ No se encontrÃ³ botÃ³n de bÃºsqueda")
            return False
        
        # 4. Hacer click en el botÃ³n
        print("\n4ï¸âƒ£ Haciendo click en botÃ³n de bÃºsqueda...")
        search_button.click()
        print("âœ… Click realizado")
        
        # 5. Esperar resultados
        print("\n5ï¸âƒ£ Esperando resultados...")
        time.sleep(8)  # Esperar mÃ¡s tiempo
        
        # 6. Analizar pÃ¡gina despuÃ©s de la bÃºsqueda
        print("\n6ï¸âƒ£ Analizando pÃ¡gina despuÃ©s de la bÃºsqueda...")
        
        # Obtener URL actual
        current_url = scraper.driver.current_url
        print(f"ğŸŒ URL actual: {current_url}")
        
        # Buscar elementos de resultado
        result_selectors = [
            ".list-group-item",
            ".item",
            ".result",
            "tr",
            ".row",
            ".col",
            ".card"
        ]
        
        for selector in result_selectors:
            elements = scraper.driver.find_elements("css selector", selector)
            if elements:
                print(f"ğŸ“‹ Elementos '{selector}': {len(elements)}")
                
                # Mostrar contenido de los primeros elementos
                for i, elem in enumerate(elements[:3]):
                    try:
                        text = elem.text.strip()
                        if text:
                            print(f"  Elemento {i+1}: {text[:100]}...")
                    except:
                        continue
        
        # 7. Buscar enlaces especÃ­ficos
        print("\n7ï¸âƒ£ Buscando enlaces especÃ­ficos...")
        
        # Buscar enlaces que contengan "detalle" o "tesis"
        links = scraper.driver.find_elements("tag name", "a")
        detail_links = []
        
        for link in links:
            try:
                href = link.get_attribute("href")
                text = link.text.strip()
                if href and ("detalle" in href.lower() or "tesis" in href.lower()):
                    detail_links.append((text, href))
            except:
                continue
        
        print(f"ğŸ”— Enlaces de detalle encontrados: {len(detail_links)}")
        for text, href in detail_links[:5]:
            print(f"  ğŸ“„ '{text}' -> {href}")
        
        # 8. Guardar HTML de la pÃ¡gina
        print("\n8ï¸âƒ£ Guardando HTML de la pÃ¡gina...")
        page_source = scraper.driver.page_source
        
        # Guardar HTML para anÃ¡lisis
        with open("logs/debug_search_results.html", "w", encoding="utf-8") as f:
            f.write(page_source)
        print("âœ… HTML guardado en logs/debug_search_results.html")
        
        # 9. Buscar patrones especÃ­ficos
        print("\n9ï¸âƒ£ Analizando patrones en HTML...")
        
        patterns = [
            "detalle/tesis",
            "tesis/",
            "tesis",
            "amparo",
            "constitucional",
            "list-group-item",
            "button-addon1_add"
        ]
        
        for pattern in patterns:
            count = page_source.lower().count(pattern.lower())
            print(f"  PatrÃ³n '{pattern}': {count} ocurrencias")
        
        print("\nâœ… Debug completado")
        
    except Exception as e:
        print(f"âŒ Error en debug: {e}")
    finally:
        scraper.close_driver()
    
    return True

def main():
    """FunciÃ³n principal"""
    success = debug_search_flow()
    
    if success:
        print("\nğŸ“‹ Resumen del debug:")
        print("âœ… Se ejecutÃ³ el flujo completo de bÃºsqueda")
        print("âœ… Se capturÃ³ la pÃ¡gina despuÃ©s de la bÃºsqueda")
        print("âœ… Se guardÃ³ el HTML para anÃ¡lisis")
        print("ğŸ“ Revisa logs/debug_search_results.html para mÃ¡s detalles")
    else:
        print("\nâŒ Error en el debug")

if __name__ == "__main__":
    main() 